{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queries import *\n",
    "import json\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('explanation.json', 'w') as f:\n",
    "    hits = query_explain('barbaresco gallina', 'ceretto', 'RED')\n",
    "    # hits = query_explain('barbaresco', 'ceretto', 'RED')\n",
    "    json.dump(hits, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bbb.json', 'w') as f:\n",
    "    bbb = query_es_clean('barbaresco gallina', 'ceretto', 'RED')\n",
    "    json.dump(bbb, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of v3 Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all datasets, and perform cleanup. The cleanup steps are:\n",
    "\n",
    " - add client column\n",
    " - remove null scores (for some reason, only laite has them)\n",
    " - convert `'ok'` column from `str` to `bool` (for some reason, only for vinessa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for element in os.listdir('./'):\n",
    "    element_path = os.path.join('./', element)\n",
    "    if not os.path.isdir(element_path) or element=='__pycache__' or '.' in element:\n",
    "        continue\n",
    "    df = pd.read_csv(os.path.join(element_path, 'v3-selection.csv'))\n",
    "\n",
    "    # cleanup\n",
    "    df['client'] = element\n",
    "    df = df.loc[~df['score'].isna()]\n",
    "    df['ok'] = df['ok'].apply(bool)\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ok = df.loc[df['ok']==True]\n",
    "df_not = df.loc[df['ok']==False]\n",
    "\n",
    "display(pd.DataFrame({'OK': df_ok['score'].describe(), 'NOT OK': df_not['score'].describe()}))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "bins = np.arange(0, 105, 2)\n",
    "df_ok['score'].hist(ax=ax, bins=bins, density=True, alpha=0.5)\n",
    "df_not['score'].hist(ax=ax, bins=bins, density=True, alpha=0.5)\n",
    "\n",
    "df_ok['score'].plot.kde(ax=ax, color='C0')\n",
    "df_not['score'].plot.kde(ax=ax, color='C1')\n",
    "\n",
    "ax.set_xlim(bins[0], bins[-1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "padding = 10\n",
    "eval_range = np.linspace(df['score'].min()-padding, df['score'].max()+padding, 1001)\n",
    "eval_step = eval_range[1] - eval_range[0]\n",
    "\n",
    "kernel_ok = gaussian_kde(df_ok['score'])\n",
    "kernel_not = gaussian_kde(df_not['score'])\n",
    "\n",
    "dist_ok = kernel_ok(eval_range)\n",
    "dist_not = kernel_not(eval_range)\n",
    "\n",
    "print(f'Area of OK: {dist_ok.sum()*eval_step:.3f}')\n",
    "print(f'Area of NOT: {dist_not.sum()*eval_step:.3f}')\n",
    "\n",
    "overlap = np.minimum(dist_ok, dist_not).sum()*eval_step\n",
    "union = np.maximum(dist_ok, dist_not).sum()*eval_step\n",
    "iou = overlap/union\n",
    "\n",
    "plt.plot(eval_range, dist_ok)\n",
    "plt.plot(eval_range, dist_not)\n",
    "\n",
    "print(f'Overlap: {overlap:.3f}')\n",
    "print(f'Union: {union:.3f}')\n",
    "print(f'IoU: {iou:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on all-selections.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from queries import query_by_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read `all-selections.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv('all-selections.csv')\n",
    "df_original.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract only the useful columns:\n",
    " - `'name'`\n",
    " - `'winery_name'`\n",
    " - `'type'`\n",
    " - `'matched_id'`\n",
    "\n",
    "Additional actions:\n",
    " - rename `'matched_id'` to `'correct_id'`.\n",
    "\n",
    "This is because the only correct match is `'matched_id'`, and the columns `'matched_name'`, `'matched_winery_name'` are incorrect. I have to build them again by querying the ElasticSearch index by id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_original.get(['name', 'winery_name', 'type', 'matched_id'])\n",
    "df = df.rename(columns={'matched_id': 'correct_id'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the correct values for `'name'`, `'winery_name'` and`'type'`:\n",
    " - Query the ElasticSearch index by id\n",
    " - generate dictionary from id to names\n",
    " - add new columns to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_by_id(list(df['correct_id']))\n",
    "display(response[0])\n",
    "\n",
    "id_mapping = {r['id']: r for r in response}\n",
    "\n",
    "df['correct_name'] = df['correct_id'].map(lambda x: id_mapping[x]['name'])\n",
    "df['correct_winery_name'] = df['correct_id'].map(lambda x: id_mapping[x]['winery_name'])\n",
    "df['correct_type'] = df['correct_id'].map(lambda x: id_mapping[x]['type'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(df['correct_type'] != df['type'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocabulary of all words\n",
    "\n",
    "TODO: get the document frequencies of all the words in the 2 vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_name = sorted(set([word for doc in df['name'].map(str.split) for word in doc]))\n",
    "vocabulary_winery_name = sorted(set([word for doc in df['winery_name'].map(str.split) for word in doc]))\n",
    "\n",
    "print(len(vocabulary_name))\n",
    "print(len(vocabulary_winery_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactoring queries\n",
    "\n",
    "TODO: modularize queries, write docstrings and move them in `queries.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def condition_name(name: str):\n",
    "    return {\n",
    "        \"multi_match\": {\n",
    "            \"query\": name,\n",
    "            \"fields\": [\n",
    "                \"name\"\n",
    "            ],\n",
    "            \"type\": \"best_fields\",\n",
    "            \"operator\": \"or\",\n",
    "                        \"fuzziness\": \"AUTO\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def condition_winery_name(winery_name: str):\n",
    "    return {\n",
    "        \"multi_match\": {\n",
    "            \"query\": winery_name,\n",
    "            \"fields\": [\n",
    "                \"winery_name^2\"\n",
    "            ],\n",
    "            \"type\": \"best_fields\",\n",
    "            \"operator\": \"or\",\n",
    "                        \"fuzziness\": \"1\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def condition_wine_type(wine_type: str):\n",
    "    return {\n",
    "        \"constant_score\": {\n",
    "            \"filter\": {\n",
    "                \"term\": {\n",
    "                    \"type.keyword\": wine_type.upper()\n",
    "                }\n",
    "            },\n",
    "            'boost': 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def query_es_2(conditions: list, num_results: int = 1):\n",
    "    url = 'https://es.vinoteqa.com/wines/_search'\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": conditions\n",
    "            }\n",
    "        },\n",
    "        \"size\": num_results\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=query)\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(f'ERROR', e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def query_es_clean_2(conditions, match_rank: int = 1):\n",
    "    response_json = query_es_2(conditions, match_rank)\n",
    "\n",
    "    if response_json is None:\n",
    "        return None, 0\n",
    "\n",
    "    hits = response_json['hits']['hits']\n",
    "    if len(hits) > match_rank-1:\n",
    "        match = hits[match_rank-1]['_source']\n",
    "        return match, hits[match_rank-1]['_score']\n",
    "    return None, 0\n",
    "\n",
    "\n",
    "def query_explain_2(conditions: list, matched_id: str = None):\n",
    "    if matched_id is None:\n",
    "        match, score = query_es_clean_2(conditions)\n",
    "        if match is None:\n",
    "            print('No match')\n",
    "            return\n",
    "        matched_id = match['id']\n",
    "\n",
    "    # perform explanation query\n",
    "    url = 'https://es.vinoteqa.com/wines/_explain/' + matched_id\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": conditions,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return requests.post(url, headers=headers, json=query).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document frequencies of each word in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_node(explanation, name: str):\n",
    "    if explanation is None:\n",
    "        return None\n",
    "\n",
    "    if explanation['description']==name:\n",
    "        return explanation['value']\n",
    "\n",
    "    for detail in explanation['details']:\n",
    "        result = find_node(detail, name)\n",
    "        if result is not None:\n",
    "            return result\n",
    "\n",
    "\n",
    "def get_explanation(word: str, index: str = 'name'):\n",
    "    explanation = None\n",
    "    if index == 'name':\n",
    "        explanation = query_explain_2(condition_name(word))\n",
    "    elif index == 'winery_name':\n",
    "        explanation = query_explain_2(condition_winery_name(word))\n",
    "    else:\n",
    "        raise ValueError(\"The index must be either 'name' or 'winery_name'\")\n",
    "\n",
    "    if explanation is not None:\n",
    "        return explanation['explanation']\n",
    "\n",
    "\n",
    "def get_document_occurrences(word: str, index: str = 'name'):\n",
    "    explanation = get_explanation(word, index)\n",
    "    if explanation is None:\n",
    "        return 0\n",
    "    return find_node(explanation, 'n, number of documents containing term')\n",
    "\n",
    "\n",
    "def get_total_documents(word: str, index: str = 'name'):\n",
    "    explanation = get_explanation(word, index)\n",
    "    if explanation is None:\n",
    "        return 0\n",
    "    return find_node(explanation, 'N, total number of documents with field')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from utils.list_manipulation import split_list_2\n",
    "from utils.time_utils import compute_eta, format_seconds\n",
    "\n",
    "def get_occurrences(vocabulary: list):\n",
    "    occurences = {}\n",
    "\n",
    "    t0 = time.time()\n",
    "    t1 = t0\n",
    "    eta = None\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        # print(f'{i} of {len(vocabulary)}, {word}'.ljust(80), end='\\r')\n",
    "\n",
    "        # ETA\n",
    "        t = time.time()\n",
    "        eta = compute_eta(t-t1, len(vocabulary)-i, eta)\n",
    "        print(f'{i} of {len(vocabulary)}, {word}'.ljust(50) +\n",
    "              f\"T/step: {t-t1:.2f}s, Elapsed: {format_seconds(t-t0)}, ETA: {format_seconds(eta)}.\",\n",
    "              end='\\r')\n",
    "        occurences[word] = get_document_occurrences(word, 'name')\n",
    "        t1 = t\n",
    "    return occurences\n",
    "\n",
    "\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def parallel_download(vocabulary, num_jobs=None):\n",
    "    if num_jobs is None:\n",
    "        num_jobs = os.cpu_count()\n",
    "    p = Pool(num_jobs)\n",
    "\n",
    "    p.map(get_occurrences, split_list_2(vocabulary, num_jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency_name = get_occurrences(vocabulary_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency_name = parallel_download(vocabulary_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vinoteqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
